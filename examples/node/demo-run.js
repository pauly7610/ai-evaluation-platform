/**
 * Demo Evaluation Run
 * 
 * This example demonstrates how to run an evaluation using the AI Evaluation Platform SDK.
 * It creates an evaluation against a demo dataset and saves the results to a JSON file.
 * 
 * Usage:
 *   npm run demo
 */

import { AIEvalClient } from '@evalai/sdk';
import fs from 'fs';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config();

async function runDemo() {
  try {
    // Initialize the client
    const client = AIEvalClient.init({ 
      apiKey: process.env.EVALAI_API_KEY 
    });

    console.log('ğŸš€ Starting evaluation...\n');

    // Create an evaluation
    const result = await client.evaluations.create({
      datasetId: 'public-demo-chatbot',
      metrics: ['factuality', 'toxicity'],
      name: 'Demo Chatbot Evaluation',
      description: 'Testing chatbot responses for accuracy and safety'
    });

    console.log('âœ… Evaluation complete!\n');
    console.log(`ğŸ“Š Overall Score: ${result.overall}`);
    console.log(`âœ“ Passed: ${result.passed}`);
    console.log(`âœ— Failed: ${result.failed}`);
    console.log(`â±ï¸  Avg Latency: ${result.avgLatency}ms`);
    console.log(`ğŸ’° Total Cost: $${result.totalCost}\n`);

    // Save results to file
    fs.writeFileSync('./demo-run.json', JSON.stringify(result, null, 2));
    console.log('ğŸ’¾ Results saved to demo-run.json');

    return result;
  } catch (error) {
    console.error('âŒ Error running evaluation:', error.message);
    process.exit(1);
  }
}

// Run the demo
runDemo();
