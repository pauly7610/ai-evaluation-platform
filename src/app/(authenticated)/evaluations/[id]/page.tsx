"use client"

import { use } from 'react'  // Add this import
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"
import { Badge } from "@/components/ui/badge"
import Link from "next/link"
import { ArrowLeft, Play, Plus, FileText, Download, Copy } from "lucide-react"
import { useSession } from "@/lib/auth-client"
import { useRouter } from "next/navigation"
import { useEffect, useState } from "react"
import { AIQualityScoreCard } from "@/components/ai-quality-score-card"
import { calculateQualityScore, type EvaluationStats } from "@/lib/ai-quality-score"
import { toast } from "sonner"
import { formatExportData, generateExportFilename, getExportDescription, validateExportData, type EvaluationType } from "@/lib/export-templates"
import { ExportModal, type ExportOptions } from "@/components/export-modal"

// Update type
type PageProps = {
  params: Promise<{ id: string }>
}

export default function EvaluationDetailPage({ params }: PageProps) {
  const { id } = use(params)  // Unwrap Promise with use()
  const { data: session, isPending } = useSession()
  const router = useRouter()
  const [evaluation, setEvaluation] = useState<any>(null)
  const [testCases, setTestCases] = useState<any[]>([])
  const [runs, setRuns] = useState<any[]>([])
  const [isLoading, setIsLoading] = useState(true)
  const [qualityScore, setQualityScore] = useState<any>(null)
  const [exportModalOpen, setExportModalOpen] = useState(false)

  useEffect(() => {
    if (!isPending && !session?.user) {
      router.push("/auth/login")
      return
    }

    if (session?.user) {
      const token = localStorage.getItem("bearer_token")
      
      // Fetch evaluation
      fetch(`/api/evaluations/${id}`, {
        headers: { Authorization: `Bearer ${token}` }
      })
        .then(res => res.json())
        .then(data => {
          if (data.error) {
            router.push("/evaluations")
          } else {
            setEvaluation(data.evaluation)
          }
        })

      // Fetch test cases
      fetch(`/api/evaluations/${id}/test-cases`, {
        headers: { Authorization: `Bearer ${token}` }
      })
        .then(res => res.json())
        .then(data => setTestCases(data.testCases || []))

      // Fetch runs
      fetch(`/api/evaluations/${id}/runs`, {
        headers: { Authorization: `Bearer ${token}` }
      })
        .then(res => res.json())
        .then(data => {
          const fetchedRuns = data.runs || []
          setRuns(fetchedRuns)
          
          // Calculate quality score from runs
          if (fetchedRuns.length > 0) {
            const latestRun = fetchedRuns[0]
            const stats: EvaluationStats = {
              totalEvaluations: latestRun.total_tests || 0,
              passedEvaluations: latestRun.passed_tests || 0,
              failedEvaluations: (latestRun.total_tests || 0) - (latestRun.passed_tests || 0),
              averageLatency: latestRun.average_latency || 1000,
              averageCost: latestRun.average_cost || 0.01,
              averageScore: latestRun.passed_tests && latestRun.total_tests 
                ? (latestRun.passed_tests / latestRun.total_tests) * 100 
                : 0,
              consistencyScore: latestRun.consistency_score || 85
            }
            const score = calculateQualityScore(stats)
            setQualityScore(score)
          }
          
          setIsLoading(false)
        })
    }
  }, [session, isPending, router, id])  // Changed from params.id to id
  
  const handleCopyResults = () => {
    if (!qualityScore || !evaluation) return
    
    const latestRun = runs[0]
    const summary = `
Evaluation Results: ${evaluation.name}
Grade: ${qualityScore.grade} (${qualityScore.overall}/100)

Summary:
- Total Tests: ${latestRun?.total_tests || 0}
- Passed: ${latestRun?.passed_tests || 0}
- Failed: ${(latestRun?.total_tests || 0) - (latestRun?.passed_tests || 0)}
- Pass Rate: ${latestRun?.total_tests ? Math.round((latestRun.passed_tests / latestRun.total_tests) * 100) : 0}%

Quality Metrics:
- Accuracy: ${qualityScore.metrics.accuracy}/100
- Safety: ${qualityScore.metrics.safety}/100
- Latency: ${qualityScore.metrics.latency}/100
- Cost: ${qualityScore.metrics.cost}/100
- Consistency: ${qualityScore.metrics.consistency}/100

Key Insights:
${qualityScore.insights.map((i: string) => `- ${i}`).join('\n')}

Recommendations:
${qualityScore.recommendations.map((r: string) => `- ${r}`).join('\n')}
    `.trim()

    navigator.clipboard.writeText(summary)
    toast.success('Results copied to clipboard!')
  }

  const handleExportWithOptions = async (options: ExportOptions): Promise<string | null> => {
    if (!qualityScore || !evaluation) return null
    
    const latestRun = runs[0]
    
    // Base export data
    const baseData = {
      evaluation: {
        id: evaluation.id,
        name: evaluation.name,
        description: evaluation.description,
        type: evaluation.type as EvaluationType,
        category: evaluation.category,
        created_at: evaluation.created_at
      },
      timestamp: new Date().toISOString(),
      summary: {
        totalTests: latestRun?.total_tests || 0,
        passed: latestRun?.passed_tests || 0,
        failed: (latestRun?.total_tests || 0) - (latestRun?.passed_tests || 0),
        passRate: latestRun?.total_tests 
          ? `${Math.round((latestRun.passed_tests / latestRun.total_tests) * 100)}%` 
          : '0%'
      },
      qualityScore: qualityScore
    }
    
    // Type-specific additional data
    const additionalData = getAdditionalExportData(evaluation.type, testCases, runs, latestRun)
    
    // Format based on template type
    const exportData = formatExportData(baseData, additionalData)
    
    // Validate export data
    const validation = validateExportData(exportData)
    if (!validation.valid) {
      console.warn('Export data incomplete:', validation.missingFields)
    }
    
    // If publishing as demo, call API
    if (options.publishAsDemo) {
      try {
        const response = await fetch(`/api/evaluations/${id}/publish`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            exportData,
            customShareId: options.customShareId,
          }),
        })
        
        if (!response.ok) {
          const error = await response.json()
          throw new Error(error.error || 'Failed to publish')
        }
        
        const result = await response.json()
        
        // Also download the file
        downloadExportFile(exportData, evaluation)
        
        return result.shareId
      } catch (error) {
        console.error('Publish error:', error)
        throw error
      }
    } else {
      // Just download the file
      downloadExportFile(exportData, evaluation)
      return null
    }
  }
  
  const downloadExportFile = (exportData: any, evaluation: any) => {
    const filename = generateExportFilename(
      evaluation.name,
      evaluation.type as EvaluationType,
      evaluation.category
    )
    
    const blob = new Blob([JSON.stringify(exportData, null, 2)], { type: 'application/json' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = filename
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
  }
  
  // Helper function to get type-specific export data
  const getAdditionalExportData = (type: string, testCases: any[], runs: any[], latestRun: any) => {
    switch (type) {
      case 'unit_test':
        return {
          testResults: testCases.map((tc: any) => ({
            id: tc.id,
            name: tc.name,
            input: tc.input,
            expected_output: tc.expected_output,
            actual_output: tc.actual_output,
            passed: tc.passed || false,
            execution_time_ms: tc.execution_time_ms,
            error_message: tc.error_message
          })),
          codeValidation: latestRun?.code_validation
        }
      
      case 'human_eval':
        return {
          evaluations: latestRun?.human_evaluations || [],
          criteria: evaluation?.human_eval_criteria || [],
          interRaterReliability: latestRun?.inter_rater_reliability
        }
      
      case 'model_eval':
        return {
          judgeEvaluations: latestRun?.judge_evaluations || [],
          judgePrompt: evaluation?.judge_prompt || '',
          judgeModel: evaluation?.judge_model || 'gpt-4',
          aggregateMetrics: latestRun?.aggregate_metrics
        }
      
      case 'ab_test':
        return {
          variants: evaluation?.variants || [],
          results: runs.map((run: any) => ({
            variant_id: run.variant_id,
            variant_name: run.variant_name,
            test_count: run.total_tests,
            success_rate: run.passed_tests / run.total_tests,
            average_latency: run.average_latency,
            average_cost: run.average_cost,
            quality_score: run.quality_score
          })),
          statisticalSignificance: latestRun?.statistical_significance,
          comparison: latestRun?.comparison
        }
      
      default:
        return {
          testResults: testCases,
          recentRuns: runs.slice(0, 5)
        }
    }
  }

  if (isPending || !session?.user || isLoading || !evaluation) {
    return null
  }

  return (
    <div className="space-y-4 sm:space-y-6">
      <div className="flex items-center gap-3 sm:gap-4">
        <Button variant="ghost" size="sm" asChild className="h-9">
          <Link href="/evaluations">
            <ArrowLeft className="mr-2 h-3.5 w-3.5 sm:h-4 sm:w-4" />
            <span className="hidden sm:inline">Back to Evaluations</span>
            <span className="sm:hidden">Back</span>
          </Link>
        </Button>
      </div>

      {/* Evaluation Header */}
      <div>
        <div className="flex flex-col sm:flex-row sm:items-start justify-between gap-3 sm:gap-4 mb-3 sm:mb-4">
          <div className="flex-1">
            <div className="flex flex-wrap items-center gap-2 sm:gap-3 mb-2">
              <h1 className="text-2xl sm:text-3xl font-bold">{evaluation.name}</h1>
              <Badge
                variant="outline"
                className={`${
                  evaluation.type === "unit_test"
                    ? "bg-blue-500/10 text-blue-500 border-blue-500/20"
                    : evaluation.type === "human_eval"
                      ? "bg-green-500/10 text-green-500 border-green-500/20"
                      : evaluation.type === "model_eval"
                        ? "bg-purple-500/10 text-purple-500 border-purple-500/20"
                        : "bg-orange-500/10 text-orange-500 border-orange-500/20"
                }`}
              >
                {evaluation.type.replace("_", " ")}
              </Badge>
            </div>
            <p className="text-sm sm:text-base text-muted-foreground">{evaluation.description || "No description provided"}</p>
          </div>
          <div className="flex gap-2 w-full sm:w-auto">
            {qualityScore && (
              <>
                <Button variant="outline" size="sm" onClick={handleCopyResults} className="flex-1 sm:flex-none">
                  <Copy className="mr-2 h-3.5 w-3.5 sm:h-4 sm:w-4" />
                  Copy
                </Button>
                <Button variant="outline" size="sm" onClick={() => setExportModalOpen(true)} className="flex-1 sm:flex-none">
                  <Download className="mr-2 h-3.5 w-3.5 sm:h-4 sm:w-4" />
                  Export
                </Button>
              </>
            )}
            <Button size="sm" className="flex-1 sm:flex-none">
              <Play className="mr-2 h-3.5 w-3.5 sm:h-4 sm:w-4" />
              Run
            </Button>
          </div>
        </div>
      </div>

      {/* Stats */}
      <div className="grid gap-3 sm:gap-4 grid-cols-3">
        <Card>
          <CardHeader className="pb-2 sm:pb-3">
            <CardTitle className="text-xs sm:text-sm font-medium">Test Cases</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-xl sm:text-2xl font-bold">{testCases.length}</div>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="pb-2 sm:pb-3">
            <CardTitle className="text-xs sm:text-sm font-medium">Total Runs</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-xl sm:text-2xl font-bold">{runs.length}</div>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="pb-2 sm:pb-3">
            <CardTitle className="text-xs sm:text-sm font-medium">Last Run</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-xs sm:text-sm">
              {runs.length > 0 ? new Date(runs[0].started_at).toLocaleDateString() : "Never"}
            </div>
          </CardContent>
        </Card>
      </div>

      {/* Quality Score Card */}
      {qualityScore && (
        <AIQualityScoreCard score={qualityScore} />
      )}

      {/* Test Cases */}
      <div>
        <div className="flex flex-col sm:flex-row sm:items-center justify-between gap-3 sm:gap-4 mb-3 sm:mb-4">
          <h2 className="text-lg sm:text-xl font-semibold">Test Cases</h2>
          <Button variant="outline" size="sm" className="w-full sm:w-auto">
            <Plus className="mr-2 h-3.5 w-3.5 sm:h-4 sm:w-4" />
            Add Test Case
          </Button>
        </div>

        {testCases.length > 0 ? (
          <div className="space-y-2 sm:space-y-3">
            {testCases.map((testCase: any) => (
              <Card key={testCase.id}>
                <CardContent className="p-3 sm:p-4">
                  <div className="flex items-start justify-between">
                    <div className="flex-1">
                      <h3 className="text-sm sm:text-base font-semibold mb-2">{testCase.name}</h3>
                      <div className="grid gap-2 sm:gap-3 md:grid-cols-2">
                        <div>
                          <p className="text-xs text-muted-foreground mb-1">Input</p>
                          <pre className="text-xs bg-muted rounded p-2 overflow-x-auto max-h-24">
                            {JSON.stringify(testCase.input, null, 2)}
                          </pre>
                        </div>
                        {testCase.expected_output && (
                          <div>
                            <p className="text-xs text-muted-foreground mb-1">Expected Output</p>
                            <pre className="text-xs bg-muted rounded p-2 overflow-x-auto max-h-24">
                              {JSON.stringify(testCase.expected_output, null, 2)}
                            </pre>
                          </div>
                        )}
                      </div>
                    </div>
                  </div>
                </CardContent>
              </Card>
            ))}
          </div>
        ) : (
          <Card>
            <CardContent className="py-12 text-center">
              <FileText className="mx-auto h-12 w-12 text-muted-foreground mb-4" />
              <h3 className="text-lg font-semibold mb-2">No test cases yet</h3>
              <p className="text-muted-foreground mb-4">Add test cases to start evaluating your AI models</p>
              <Button>
                <Plus className="mr-2 h-4 w-4" />
                Add Test Case
              </Button>
            </CardContent>
          </Card>
        )}
      </div>

      {/* Recent Runs */}
      <div>
        <h2 className="mb-4 text-xl font-semibold">Recent Runs</h2>
        {runs.length > 0 ? (
          <div className="space-y-3">
            {runs.map((run: any) => (
              <Card key={run.id}>
                <CardContent className="p-4">
                  <div className="flex items-center justify-between">
                    <div>
                      <div className="flex items-center gap-3 mb-1">
                        <Badge
                          variant="outline"
                          className={`${
                            run.status === "completed"
                              ? "bg-green-500/10 text-green-500 border-green-500/20"
                              : run.status === "running"
                                ? "bg-blue-500/10 text-blue-500 border-blue-500/20"
                                : run.status === "failed"
                                  ? "bg-red-500/10 text-red-500 border-red-500/20"
                                  : "bg-gray-500/10 text-gray-500 border-gray-500/20"
                          }`}
                        >
                          {run.status}
                        </Badge>
                        <span className="text-sm text-muted-foreground">{new Date(run.started_at).toLocaleString()}</span>
                      </div>
                      {run.status === "completed" && (
                        <p className="text-sm">
                          {run.passed_tests} / {run.total_tests} tests passed
                        </p>
                      )}
                    </div>
                    <Button variant="outline" size="sm">
                      View Results
                    </Button>
                  </div>
                </CardContent>
              </Card>
            ))}
          </div>
        ) : (
          <Card>
            <CardContent className="py-12 text-center">
              <p className="text-muted-foreground">No runs yet</p>
            </CardContent>
          </Card>
        )}
      </div>

      {/* Export Modal */}
      <ExportModal
        open={exportModalOpen}
        onOpenChange={setExportModalOpen}
        evaluationName={evaluation?.name || 'Evaluation'}
        onExport={handleExportWithOptions}
      />
    </div>
  )
}
